{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate, zeros\n",
    "from scipy.linalg import toeplitz\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib as mat\n",
    "mat.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)    # reproducible\n",
    "mat.use(\"TkAgg\")\n",
    "\n",
    "hidden_siz = 50\n",
    "hidden_lay = 1\n",
    "\n",
    "LR = 0.02           # learning rate\n",
    "class LSNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSNN1, self).__init__()\n",
    "        self.lstm = nn.LSTM(  \n",
    "            input_size=1,\n",
    "            hidden_size=hidden_siz,    \n",
    "            num_layers=hidden_lay,      \n",
    "            batch_first=True,\n",
    "\n",
    "        )\n",
    "        self.hidden = (torch.autograd.Variable(torch.zeros(hidden_lay, 1, hidden_siz)),torch.autograd.Variable(torch.zeros(hidden_lay, 1, hidden_siz)))\n",
    "        self.out = nn.Linear(hidden_siz, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size)\n",
    "        # r_out (batch, time_step, output_size)\n",
    "        r_out,self.hidden= self.lstm(x,self.hidden)\n",
    "        self.hidden=(Variable(self.hidden[0]),Variable(self.hidden[1]))\n",
    "        outs = []\n",
    "        #print(r_out.size())\n",
    "        for time_step in range(33):\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "        return torch.stack(outs, dim=1)\n",
    "\n",
    "    \n",
    "class LSNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSNN2, self).__init__()\n",
    "        self.lstm = nn.LSTM(  \n",
    "            input_size=1,\n",
    "            hidden_size=hidden_siz,    \n",
    "            num_layers=hidden_lay,      \n",
    "            batch_first=True,\n",
    "\n",
    "        )\n",
    "        self.hidden = (torch.autograd.Variable(torch.zeros(hidden_lay, 1, hidden_siz)),torch.autograd.Variable(torch.zeros(hidden_lay, 1, hidden_siz)))\n",
    "        self.out = nn.Linear(hidden_siz, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size)\n",
    "        # r_out (batch, time_step, output_size)\n",
    "        r_out,self.hidden= self.lstm(x,self.hidden)\n",
    "        self.hidden=(Variable(self.hidden[0]),Variable(self.hidden[1]))\n",
    "        outs = []\n",
    "        for time_step in range(100):\n",
    "            if(time_step>=33 and time_step<66):\n",
    "                outs.append(self.out(r_out[:, time_step, :]))\n",
    "        return torch.stack(outs, dim=1)\n",
    "\n",
    "class LSNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSNN3, self).__init__()\n",
    "        self.lstm = nn.LSTM(  \n",
    "            input_size=1,\n",
    "            hidden_size=hidden_siz,    \n",
    "            num_layers=hidden_lay,      \n",
    "            batch_first=True,\n",
    "\n",
    "        )\n",
    "        self.hidden = (torch.autograd.Variable(torch.zeros(hidden_lay, 1, hidden_siz)),torch.autograd.Variable(torch.zeros(hidden_lay, 1, hidden_siz)))\n",
    "        self.out = nn.Linear(hidden_siz, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size)\n",
    "        # r_out (batch, time_step, output_size)\n",
    "        r_out,self.hidden= self.lstm(x,self.hidden)\n",
    "        self.hidden=(Variable(self.hidden[0]),Variable(self.hidden[1]))\n",
    "        outs = []\n",
    "        for time_step in range(67):\n",
    "            if(time_step>=33 and time_step<67):\n",
    "                outs.append(self.out(r_out[:, time_step, :]))\n",
    "        return torch.stack(outs, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstmNN1 = LSNN1()\n",
    "lstmNN2 = LSNN2()\n",
    "lstmNN3 = LSNN3()\n",
    "optimizer1 = torch.optim.Adam(lstmNN1.parameters(), lr=LR)  # optimize all rnn parameters\n",
    "optimizer2 = torch.optim.Adam(lstmNN2.parameters(), lr=LR)\n",
    "optimizer3 = torch.optim.Adam(lstmNN3.parameters(), lr=LR)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "\n",
    "loss_list1 = []\n",
    "loss_list2 = []\n",
    "loss_list3 = []\n",
    "prediction_list1 = []\n",
    "prediction_list2 = []\n",
    "prediction_list3 = []\n",
    "\n",
    "steps = np.linspace(0, 100, 100, dtype=np.float32)\n",
    "\n",
    "for step in range(98):\n",
    "    \n",
    "    if step == 0:\n",
    "        x_np1 = toeplitz(concatenate([[1.], zeros(99)]),concatenate([[1.,1.,1.], zeros(97)]))[step: step + 1, :33]\n",
    "        x_np2 = toeplitz(concatenate([[1.], zeros(99)]),concatenate([[1.,1.,1.], zeros(97)]))[step: step + 1, 33: 66]      \n",
    "        x_np3 = toeplitz(concatenate([[1.], zeros(99)]),concatenate([[1.,1.,1.], zeros(97)]))[step: step + 1, 66:]\n",
    "        y_np1 = toeplitz(concatenate([[1.], zeros(99)]),concatenate([[1.,1.,1.], zeros(97)]))[step + 1: step + 2, :33]\n",
    "        y_np2 = toeplitz(concatenate([[1.], zeros(99)]),concatenate([[1.,1.,1.], zeros(97)]))[step + 1: step + 2, 33: 66]\n",
    "        y_np3 = toeplitz(concatenate([[1.], zeros(99)]),concatenate([[1.,1.,1.], zeros(97)]))[step + 1: step + 2, 66:]\n",
    "        x1 = Variable(torch.from_numpy(np.append(x_np1,x_np2)).float())  # shape (batch, time_step, input_size)\n",
    "        y1 = Variable(torch.from_numpy(y_np1).float())\n",
    "        x2 = Variable(torch.from_numpy(np.append(np.append(x_np1,x_np2),x_np3)).float())  # shape (batch, time_step, input_size)\n",
    "        y2 = Variable(torch.from_numpy(y_np2).float())\n",
    "        x3 = Variable(torch.from_numpy(np.append(x_np2,x_np3)).float())  # shape (batch, time_step, input_size)\n",
    "        y3 = Variable(torch.from_numpy(y_np3).float())\n",
    "    else:\n",
    "        y_np1 = toeplitz(concatenate([[1.], zeros(99)]),concatenate([[1.,1.,1.], zeros(97)]))[step + 1: step + 2, :33]\n",
    "        y_np2 = toeplitz(concatenate([[1.], zeros(99)]),concatenate([[1.,1.,1.], zeros(97)]))[step + 1: step + 2, 33: 66]\n",
    "        y_np3 = toeplitz(concatenate([[1.], zeros(99)]),concatenate([[1.,1.,1.], zeros(97)]))[step + 1: step + 2, 66:]\n",
    "        x1 = Variable(torch.from_numpy(np.append(prediction1.data.view(33).numpy(),prediction2.data.view(33).numpy())).float())  # shape (batch, time_step, input_size)\n",
    "        y1 = Variable(torch.from_numpy(y_np1).float())\n",
    "        x2 = Variable(torch.from_numpy(np.append(np.append(prediction1.data.view(33).numpy(),prediction2.data.view(33).numpy()),prediction3.data.view(34).numpy())).float())  # shape (batch, time_step, input_size)\n",
    "        y2 = Variable(torch.from_numpy(y_np2).float())\n",
    "        x3 = Variable(torch.from_numpy(np.append(prediction2.data.view(33).numpy(),prediction3.data.view(34).numpy())).float())  # shape (batch, time_step, input_size)\n",
    "        y3 = Variable(torch.from_numpy(y_np3).float())\n",
    "    \n",
    "    #print(x_np1.size, x_np2.size,x_np3.size)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    x1 = x1.view(1,66,1)\n",
    "    y1 = y1.view(1,33,1)\n",
    "    x2 = x2.view(1,100,1)\n",
    "    y2 = y2.view(1,33,1)\n",
    "    x3 = x3.view(1,67,1)\n",
    "    y3 = y3.view(1,34,1)\n",
    "    \n",
    "    prediction1 = lstmNN1(x1)\n",
    "    \n",
    "    prediction_list1.append(prediction1.data.view(33).numpy())    \n",
    "    prediction2 = lstmNN2(x2)\n",
    "    #print(\"x2 \",x2.size())\n",
    "    #print(\"prediction2 \",prediction2.size())\n",
    "    prediction_list2.append(prediction2.data.view(33).numpy())    \n",
    "    prediction3 = lstmNN3(x3)\n",
    "    prediction_list3.append(prediction3.data.view(34).numpy())\n",
    "\n",
    "    loss1 = loss_func(prediction1, y1)\n",
    "    loss_list1.append(loss1)\n",
    "    loss2 = loss_func(prediction2, y2)\n",
    "    loss_list2.append(loss2)\n",
    "    loss3 = loss_func(prediction3, y3)\n",
    "    loss_list3.append(loss3)\n",
    "    \n",
    "    #train_loss += loss*X.size(0)\n",
    "    \n",
    "    optimizer1.zero_grad()               # clear gradients for this training step\n",
    "    optimizer2.zero_grad()\n",
    "    optimizer3.zero_grad()\n",
    "    \n",
    "    loss1.backward()                     # backpropagation, compute gradients\n",
    "    loss2.backward()\n",
    "    loss3.backward()\n",
    "    \n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "    optimizer3.step()\n",
    "\n",
    "    for i in range(20):\n",
    "    #while(loss_func(prediction1, x1[:,:33,:]) > 0.01 or loss_func(prediction2, x2[:,33: 66,:])>0.01 or loss_func(prediction3, x3[:,66: ,:])>0.01):\n",
    "        x1= Variable(torch.from_numpy(np.append(x1[:,:33,:].data.view(33).numpy(),prediction2.data.view(33).numpy())).float())\n",
    "        x2= Variable(torch.from_numpy(np.append(np.append(prediction1.data.view(33).numpy(),x2[:,33:66,:].data.view(33).numpy()),prediction3.data.view(34).numpy())).float())\n",
    "        x3= Variable(torch.from_numpy(np.append(prediction2.data.view(33).numpy(),x3[:,33:,:].data.view(34).numpy())).float())\n",
    "        x1 = x1.view(1,66,1)\n",
    "        x2 = x2.view(1,100,1)\n",
    "        x3 = x3.view(1,67,1)\n",
    "        #print(loss_func(prediction1, x1[:,:33,:]))\n",
    "        prediction1 = lstmNN1(x1)\n",
    "    \n",
    "        prediction_list1.append(prediction1.data.view(33).numpy())    \n",
    "        prediction2 = lstmNN2(x2)\n",
    "        #print(\"x2 \",x2.size())\n",
    "        #print(\"prediction2 \",prediction2.size())\n",
    "        prediction_list2.append(prediction2.data.view(33).numpy())    \n",
    "        prediction3 = lstmNN3(x3)\n",
    "        prediction_list3.append(prediction3.data.view(34).numpy())\n",
    "\n",
    "        loss1 = loss_func(prediction1, y1)\n",
    "        loss_list1.append(loss1)\n",
    "        loss2 = loss_func(prediction2, y2)\n",
    "        loss_list2.append(loss2)\n",
    "        loss3 = loss_func(prediction3, y3)\n",
    "        loss_list3.append(loss3)\n",
    "    \n",
    "        #train_loss += loss*X.size(0)\n",
    "    \n",
    "        optimizer1.zero_grad()               # clear gradients for this training step\n",
    "        optimizer2.zero_grad()\n",
    "        optimizer3.zero_grad()\n",
    "    \n",
    "        loss1.backward()                     # backpropagation, compute gradients\n",
    "        loss2.backward()\n",
    "        loss3.backward()\n",
    "    \n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        optimizer3.step()\n",
    "\n",
    "        # apply gradients\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.figsize=(20, 10)\n",
    "    plt.ion()\n",
    "    plt.title(step,fontsize=24)\n",
    "    plt.plot(steps, np.append(np.append(y_np1,y_np2),y_np3).flatten(), 'r-')\n",
    "    plt.plot(steps, np.append(np.append(prediction1.data.numpy(),prediction2.data.numpy()),prediction3.data.numpy()).flatten(), 'b-')\n",
    "    #plt.legend()\n",
    "    plt.draw()\n",
    "    plt.pause(0.01)\n",
    "    plt.clf()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
